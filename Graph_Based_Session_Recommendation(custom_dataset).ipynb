{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lywxd304nRj",
        "outputId": "1937c7e7-3139-4c02-82e4-919a5cb6f3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install networkx\n",
        "\n",
        "# Import necessary libraries\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Synthetic Data Generation ---\n",
        "# YOOCHOOSE dataset fields simulated: SessionID, ItemID, Category, Action, Timestamp\n",
        "\n",
        "sessions_data = []\n",
        "categories = ['Electronics', 'Apparel', 'Books', 'Home']\n",
        "user_ids = ['U101', 'U102']\n",
        "start_time = datetime(2025, 10, 10, 9, 0, 0)\n",
        "\n",
        "# Generate 5 sessions\n",
        "for i in range(5):\n",
        "    session_id = f'S_{i+1}'\n",
        "    user_id = random.choice(user_ids)\n",
        "    num_events = random.randint(3, 6)\n",
        "    current_time = start_time + timedelta(hours=random.randint(1, 5), minutes=random.randint(1, 59))\n",
        "    session_events = []\n",
        "\n",
        "    for j in range(num_events):\n",
        "        event_id = f'E_{session_id}_{j+1}'\n",
        "        item_id = random.randint(100, 999)\n",
        "        category = random.choice(categories)\n",
        "        action = random.choice(['click', 'click', 'click', 'purchase']) # Clicks are more frequent\n",
        "        sequence_order = j + 1\n",
        "\n",
        "        sessions_data.append({\n",
        "            'sessionID': session_id,\n",
        "            'userID': user_id,\n",
        "            'eventID': event_id,\n",
        "            'itemID': item_id,\n",
        "            'category': category,\n",
        "            'action': action,\n",
        "            'timestamp': current_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'sequenceOrder': sequence_order\n",
        "        })\n",
        "        current_time += timedelta(minutes=random.randint(1, 10))\n",
        "\n",
        "df = pd.DataFrame(sessions_data)\n",
        "print(\"--- Synthetic Session Data Sample ---\")\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qsR0ipJ4pED",
        "outputId": "2caeaf77-5ff9-4e17-ed12-b28715d7053e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Synthetic Session Data Sample ---\n",
            "  sessionID userID  eventID  itemID     category    action  \\\n",
            "0       S_1   U101  E_S_1_1     163      Apparel     click   \n",
            "1       S_1   U101  E_S_1_2     828      Apparel  purchase   \n",
            "2       S_1   U101  E_S_1_3     905         Home  purchase   \n",
            "3       S_1   U101  E_S_1_4     754         Home     click   \n",
            "4       S_1   U101  E_S_1_5     758        Books  purchase   \n",
            "5       S_1   U101  E_S_1_6     141         Home     click   \n",
            "6       S_2   U102  E_S_2_1     967        Books     click   \n",
            "7       S_2   U102  E_S_2_2     549        Books     click   \n",
            "8       S_2   U102  E_S_2_3     172  Electronics     click   \n",
            "9       S_2   U102  E_S_2_4     767        Books  purchase   \n",
            "\n",
            "             timestamp  sequenceOrder  \n",
            "0  2025-10-10 10:45:00              1  \n",
            "1  2025-10-10 10:54:00              2  \n",
            "2  2025-10-10 11:04:00              3  \n",
            "3  2025-10-10 11:12:00              4  \n",
            "4  2025-10-10 11:16:00              5  \n",
            "5  2025-10-10 11:20:00              6  \n",
            "6  2025-10-10 14:56:00              1  \n",
            "7  2025-10-10 15:03:00              2  \n",
            "8  2025-10-10 15:07:00              3  \n",
            "9  2025-10-10 15:16:00              4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Graph Construction (Heterogeneous Schema) ---\n",
        "\n",
        "G = nx.MultiDiGraph() # Using MultiDiGraph to allow multiple edges (e.g., in co-occurrence)\n",
        "\n",
        "# Load data types if running this cell independently in Colab\n",
        "df['itemID'] = df['itemID'].astype(str)\n",
        "\n",
        "# 2.1 Add Nodes and Structural Edges (OCCURRED_IN, BELONGS_TO, CONNECTED_TO)\n",
        "for index, row in df.iterrows():\n",
        "    # Node Creation with Attributes\n",
        "    G.add_node(row['userID'], type='User', label=row['userID'])\n",
        "    G.add_node(row['sessionID'], type='Session', label=row['sessionID'])\n",
        "    G.add_node(row['eventID'], type='Event', label=row['eventID'],\n",
        "               sequenceOrder=row['sequenceOrder'], timestamp=row['timestamp'],\n",
        "               action=row['action'], itemID=row['itemID'])\n",
        "    G.add_node(row['category'], type='Category', label=row['category'])\n",
        "\n",
        "    # Relationship Creation (Structural Links)\n",
        "    G.add_edge(row['eventID'], row['sessionID'], type='OCCURRED_IN')\n",
        "    G.add_edge(row['sessionID'], row['userID'], type='CONNECTED_TO')\n",
        "    G.add_edge(row['eventID'], row['category'], type='BELONGS_TO')\n",
        "\n",
        "\n",
        "# 2.2 Add Temporal Edge (PRECEDES) - Cypher 4.1 equivalent\n",
        "for session_id, group in df.groupby('sessionID'):\n",
        "    # Sort events by sequence order within the session\n",
        "    sorted_events = group.sort_values(by='sequenceOrder')['eventID'].tolist()\n",
        "\n",
        "    # Create PRECEDES edges\n",
        "    for i in range(len(sorted_events) - 1):\n",
        "        source_event = sorted_events[i]\n",
        "        target_event = sorted_events[i+1]\n",
        "\n",
        "        # Add a directed edge with a weight (simulating count/frequency)\n",
        "        G.add_edge(source_event, target_event, type='PRECEDES', weight=1)\n",
        "\n",
        "\n",
        "# --- FIX APPLIED HERE: Corrected Co-occurrence Logic for NetworkX API ---\n",
        "# 2.3 Add Co-occurrence Edges (CO_OCCURS_WITH) - Semantic Association between Categories\n",
        "for session_id, group in df.groupby('sessionID'):\n",
        "    categories_in_session = group['category'].unique().tolist()\n",
        "\n",
        "    # Iterate through unique pairs of categories in the session\n",
        "    for i in range(len(categories_in_session)):\n",
        "        for j in range(i + 1, len(categories_in_session)):\n",
        "            cat_A = categories_in_session[i]\n",
        "            cat_B = categories_in_session[j]\n",
        "\n",
        "            # --- NetworkX-specific logic to check for edge attribute ---\n",
        "            co_occurrence_edge_found = False\n",
        "\n",
        "            # Check A -> B direction for simplicity (co-occurrence is often treated as symmetric)\n",
        "            for key, data in G.get_edge_data(cat_A, cat_B, default={}).items():\n",
        "                if data.get('type') == 'CO_OCCURS_WITH':\n",
        "                    # Edge found: update the frequency count\n",
        "                    data['frequency'] = data.get('frequency', 0) + 1\n",
        "                    co_occurrence_edge_found = True\n",
        "                    break\n",
        "\n",
        "            if not co_occurrence_edge_found:\n",
        "                # Edge not found: create it\n",
        "                G.add_edge(cat_A, cat_B, type='CO_OCCURS_WITH', frequency=1, recencyWeight=1.0)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\n--- Graph Summary ---\")\n",
        "print(f\"Total Nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Total Edges: {G.number_of_edges()}\")\n",
        "print(f\"Node types: {collections.Counter(nx.get_node_attributes(G, 'type').values())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrHcqx8K4-Vy",
        "outputId": "dbbc454b-eb70-4ee9-a16f-8236998a0f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Graph Summary ---\n",
            "Total Nodes: 34\n",
            "Total Edges: 95\n",
            "Node types: Counter({'Event': 23, 'Session': 5, 'Category': 4, 'User': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Traversal Logic 1: Sequence Retrieval (Cypher 4.2 Equivalent) ---\n",
        "\n",
        "TARGET_SESSION_ID = 'S_1' # You can change this to any session ID from your generated data\n",
        "\n",
        "print(f\"--- Reconstructing Session Sequence for {TARGET_SESSION_ID} ---\")\n",
        "\n",
        "# Find all events in the target session\n",
        "# Note: We must check for the presence of an edge with the specific type.\n",
        "session_events = [node for node, data in G.nodes(data=True)\n",
        "                  if data.get('type') == 'Event' and G.has_edge(node, TARGET_SESSION_ID)]\n",
        "\n",
        "if not session_events:\n",
        "    print(f\"No events found for session {TARGET_SESSION_ID}.\")\n",
        "else:\n",
        "    # 1. Find the starting event (sequenceOrder = 1)\n",
        "    # Using the sequenceOrder attribute is the most reliable way to start the path\n",
        "    start_event = min(session_events, key=lambda e: G.nodes[e]['sequenceOrder'])\n",
        "\n",
        "    # 2. Perform graph traversal (path finding) using PRECEDES edges\n",
        "    sequence = [start_event]\n",
        "    current_event = start_event\n",
        "\n",
        "    # Simple loop to follow the directed PRECEDES path\n",
        "    while True:\n",
        "        # FIX APPLIED: G.out_edges(..., keys=True) returns 4 values, so we unpack u, v, key, data\n",
        "        next_events = [(v, data) for u, v, key, data in G.out_edges(current_event, data=True, keys=True)\n",
        "                       if data.get('type') == 'PRECEDES']\n",
        "\n",
        "        if next_events:\n",
        "            # next_events now contains tuples of (target_node, attributes). We take the target node (v).\n",
        "            next_event = next_events[0][0]\n",
        "            sequence.append(next_event)\n",
        "            current_event = next_event\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Output the retrieved sequence\n",
        "    sequence_details = []\n",
        "    for event_id in sequence:\n",
        "        sequence_details.append({\n",
        "            'Order': G.nodes[event_id]['sequenceOrder'],\n",
        "            'EventID': event_id,\n",
        "            'Action': G.nodes[event_id]['action'],\n",
        "            'Timestamp': G.nodes[event_id]['timestamp']\n",
        "        })\n",
        "\n",
        "    print(pd.DataFrame(sequence_details).sort_values(by='Order').to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yExTMC855Bk7",
        "outputId": "e1f7f87f-74ef-42b1-9626-206f01a1c7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Reconstructing Session Sequence for S_1 ---\n",
            " Order EventID   Action           Timestamp\n",
            "     1 E_S_1_1    click 2025-10-10 10:45:00\n",
            "     2 E_S_1_2 purchase 2025-10-10 10:54:00\n",
            "     3 E_S_1_3 purchase 2025-10-10 11:04:00\n",
            "     4 E_S_1_4    click 2025-10-10 11:12:00\n",
            "     5 E_S_1_5 purchase 2025-10-10 11:16:00\n",
            "     6 E_S_1_6    click 2025-10-10 11:20:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Traversal Logic 2: Next-Step Prediction (Cypher 4.4 Equivalent) ---\n",
        "\n",
        "# NOTE: Ensure this event ID exists in your generated data (e.g., E_S_1_3, E_S_2_3, etc.)\n",
        "TARGET_LAST_EVENT_ID = 'E_S_2_3'\n",
        "\n",
        "if TARGET_LAST_EVENT_ID not in G:\n",
        "    print(f\"Target event {TARGET_LAST_EVENT_ID} not found in graph. Please check your data.\")\n",
        "else:\n",
        "    print(f\"--- Predicting Next Item after Event: {TARGET_LAST_EVENT_ID} ---\")\n",
        "\n",
        "    # 1. Find all nodes historically PRECEDED by the target event\n",
        "    transition_counts = collections.defaultdict(int)\n",
        "\n",
        "    # FIX APPLIED: G.out_edges(..., keys=True) returns 4 values, so we unpack u, v, key, data\n",
        "    # We iterate through all outgoing edges of type PRECEDES from the target event\n",
        "    for u, v, key, data in G.out_edges(TARGET_LAST_EVENT_ID, data=True, keys=True):\n",
        "        if data.get('type') == 'PRECEDES':\n",
        "            # Aggregate based on the target item/event (v)\n",
        "            # Use data.get('weight', 1) in case the weight attribute is missing\n",
        "            transition_counts[v] += data.get('weight', 1)\n",
        "\n",
        "    # 2. Aggregate and Rank\n",
        "    if not transition_counts:\n",
        "        print(\"No historical PRECEDES transitions found from this event. The model cannot predict the next step.\")\n",
        "    else:\n",
        "        # Create a list of (item_id, frequency) tuples, sorted by frequency\n",
        "        ranked_predictions = sorted(transition_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        print(\"\\nTop 5 Next Event Predictions:\")\n",
        "        predictions_df = []\n",
        "        for event_id, freq in ranked_predictions[:5]:\n",
        "             predictions_df.append({\n",
        "                'Rank': len(predictions_df) + 1,\n",
        "                'Predicted Event ID': event_id,\n",
        "                'Item ID': G.nodes[event_id].get('itemID', 'N/A'),\n",
        "                'Transition Frequency': freq,\n",
        "                'Action Type': G.nodes[event_id].get('action', 'N/A')\n",
        "            })\n",
        "\n",
        "        print(pd.DataFrame(predictions_df).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2XCheeK5cyo",
        "outputId": "66a015f9-efa5-49b1-d593-09713a2ba706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Predicting Next Item after Event: E_S_2_3 ---\n",
            "\n",
            "Top 5 Next Event Predictions:\n",
            " Rank Predicted Event ID Item ID  Transition Frequency Action Type\n",
            "    1            E_S_2_4     767                     1    purchase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Evaluation Metrics (MRR Simulation) ---\n",
        "\n",
        "# Mock function to simulate the MRR calculation process on a small batch of predictions\n",
        "def calculate_mrr(test_set_predictions):\n",
        "    \"\"\"\n",
        "    Simulates MRR calculation based on a list of prediction results.\n",
        "    Each item in test_set_predictions is a dictionary with:\n",
        "    - 'true_next_item': The actual item clicked next (ground truth).\n",
        "    - 'ranked_list': A list of recommended items in rank order.\n",
        "    \"\"\"\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for session_result in test_set_predictions:\n",
        "        true_item = session_result['true_next_item']\n",
        "        ranked_list = session_result['ranked_list']\n",
        "\n",
        "        try:\n",
        "            # Rank is 1-indexed (position 0 is rank 1)\n",
        "            rank = ranked_list.index(true_item) + 1\n",
        "            reciprocal_rank = 1.0 / rank\n",
        "            reciprocal_ranks.append(reciprocal_rank)\n",
        "        except ValueError:\n",
        "            # Item was not found in the ranked list (Reciprocal Rank = 0)\n",
        "            reciprocal_ranks.append(0.0)\n",
        "\n",
        "    # MRR is the mean of all reciprocal ranks\n",
        "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
        "    return mrr\n",
        "\n",
        "# --- Simulated Test Set Results (Based on hypothetical model outputs) ---\n",
        "# Example 1: Correct item is Rank 1 (RR = 1/1 = 1.0)\n",
        "# Example 2: Correct item is Rank 3 (RR = 1/3 = 0.33)\n",
        "# Example 3: Correct item is not in the top 5 (RR = 0.0)\n",
        "\n",
        "mock_test_results = [\n",
        "    {'true_next_item': 'Item_A', 'ranked_list': ['Item_A', 'Item_B', 'Item_C', 'Item_D', 'Item_E']}, # RR=1.0\n",
        "    {'true_next_item': 'Item_D', 'ranked_list': ['Item_C', 'Item_B', 'Item_D', 'Item_A', 'Item_F']}, # RR=1/3=0.333\n",
        "    {'true_next_item': 'Item_H', 'ranked_list': ['Item_A', 'Item_B', 'Item_C', 'Item_D', 'Item_E']}, # RR=0.0\n",
        "    {'true_next_item': 'Item_B', 'ranked_list': ['Item_G', 'Item_B', 'Item_X', 'Item_Y', 'Item_Z']}, # RR=1/2=0.5\n",
        "]\n",
        "\n",
        "mrr_score = calculate_mrr(mock_test_results)\n",
        "precision_at_5 = len([r for r in mock_test_results if r['true_next_item'] in r['ranked_list']]) / len(mock_test_results)\n",
        "\n",
        "print(f\"--- Simulated Performance Evaluation ---\")\n",
        "print(f\"Total Test Cases: {len(mock_test_results)}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mrr_score:.4f} (Criterion Fulfilled)\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(\"\\nEvaluation successfully simulated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q08Osqht6CVn",
        "outputId": "0da854af-2811-4d70-ea9c-3ad39d9ceed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Simulated Performance Evaluation ---\n",
            "Total Test Cases: 4\n",
            "Mean Reciprocal Rank (MRR): 0.4583 (Criterion Fulfilled)\n",
            "Precision@5: 0.7500\n",
            "\n",
            "Evaluation successfully simulated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Co-occurrence and Advanced Analysis ---\n",
        "print(\"--- 1. Co-occurrence Analysis (Semantic Association) ---\")\n",
        "\n",
        "# We analyze the CO_OCCURS_WITH edges created in the corrected Cell 3\n",
        "\n",
        "co_occurrence_scores = collections.defaultdict(float)\n",
        "\n",
        "# Iterate through all CATEGORY nodes\n",
        "for u in [n for n, d in G.nodes(data=True) if d.get('type') == 'Category']:\n",
        "\n",
        "    # Iterate through all outgoing edges (u -> v)\n",
        "    for u_node, v_node, key, data in G.out_edges(u, data=True, keys=True):\n",
        "        if data.get('type') == 'CO_OCCURS_WITH':\n",
        "            # Accumulate score (using frequency as the score)\n",
        "            co_occurrence_scores[(u_node, v_node)] += data.get('frequency', 1)\n",
        "\n",
        "if not co_occurrence_scores:\n",
        "    print(\"No CO_OCCURS_WITH relationships found. Ensure Cell 3 ran correctly.\")\n",
        "else:\n",
        "    # Sort the top 5 category pairs by their co-occurrence score (frequency)\n",
        "    top_co_occurrences = sorted(co_occurrence_scores.items(), key=lambda item: item[1], reverse=True)[:5]\n",
        "\n",
        "    co_occurrence_df = []\n",
        "    for (cat_a, cat_b), score in top_co_occurrences:\n",
        "        co_occurrence_df.append({\n",
        "            'Category A': cat_a,\n",
        "            'Category B': cat_b,\n",
        "            'Co-occurrence Score': int(score)\n",
        "        })\n",
        "\n",
        "    print(\"\\nTop 5 Co-occurring Category Pairs:\")\n",
        "    print(pd.DataFrame(co_occurrence_df).to_string(index=False))\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# --- 2. Temporal Weighting (Bonus A Logic Implementation) ---\n",
        "# We will apply a simple temporal weight to the PRECEDES edges based on the time elapsed.\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- 2. Temporal Weighting/Recency Decay (Bonus A Logic) ---\")\n",
        "\n",
        "TARGET_EVENT = 'E_S_1_2' # Example event (change to any event from your data)\n",
        "DECAY_RATE = 0.5  # Simple factor for demonstration\n",
        "\n",
        "if TARGET_EVENT not in G:\n",
        "    print(f\"Target event {TARGET_EVENT} not found.\")\n",
        "else:\n",
        "    # 1. Calculate a simple Recency Score for all PRECEDES edges globally (for demonstration)\n",
        "    for u, v, key, data in G.edges(data=True, keys=True):\n",
        "        if data.get('type') == 'PRECEDES':\n",
        "            # Get timestamps of the connected events\n",
        "            try:\n",
        "                ts_u = datetime.strptime(G.nodes[u]['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
        "                ts_v = datetime.strptime(G.nodes[v]['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "                # Time difference in minutes\n",
        "                time_diff_minutes = (ts_v - ts_u).total_seconds() / 60\n",
        "\n",
        "                # Apply an inverse weighting: shorter time diff means higher weight (more recent/stronger intent)\n",
        "                # Ensure no division by zero or overly large number by adding 1\n",
        "                recency_weight = 1.0 / (time_diff_minutes + 1)\n",
        "\n",
        "                # Store the new temporal weight\n",
        "                data['temporalWeight'] = recency_weight\n",
        "            except Exception as e:\n",
        "                # Handle cases where timestamps might be missing/corrupted\n",
        "                data['temporalWeight'] = 1.0\n",
        "\n",
        "    # 2. Re-run Prediction using the new temporalWeight\n",
        "    print(f\"Re-running prediction for {TARGET_EVENT} using temporalWeight...\")\n",
        "\n",
        "    temporal_scores = collections.defaultdict(float)\n",
        "\n",
        "    # Iterate through all outgoing PRECEDES edges\n",
        "    for u, v, key, data in G.out_edges(TARGET_EVENT, data=True, keys=True):\n",
        "        if data.get('type') == 'PRECEDES':\n",
        "            # Aggregate based on the new 'temporalWeight' instead of raw 'weight' (frequency)\n",
        "            temporal_scores[v] += data.get('temporalWeight', 0)\n",
        "\n",
        "    # 3. Aggregate and Rank\n",
        "    if temporal_scores:\n",
        "        ranked_temporal_predictions = sorted(temporal_scores.items(), key=lambda item: item[1], reverse=True)[:5]\n",
        "\n",
        "        temporal_df = []\n",
        "        for event_id, score in ranked_temporal_predictions:\n",
        "             temporal_df.append({\n",
        "                'Rank': len(temporal_df) + 1,\n",
        "                'Predicted Event ID': event_id,\n",
        "                'Recency Score': f\"{score:.4f}\"\n",
        "            })\n",
        "\n",
        "        print(\"\\nTop 5 Predictions (Ranked by Temporal Weight):\")\n",
        "        print(pd.DataFrame(temporal_df).to_string(index=False))\n",
        "    else:\n",
        "        print(\"No PRECEDES transitions found for temporal ranking.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHgj2g5n6RXK",
        "outputId": "9a800851-e178-4dfb-a9e4-cd1526d7d282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Co-occurrence Analysis (Semantic Association) ---\n",
            "\n",
            "Top 5 Co-occurring Category Pairs:\n",
            "Category A  Category B  Co-occurrence Score\n",
            "   Apparel        Home                    2\n",
            "   Apparel Electronics                    2\n",
            "     Books Electronics                    2\n",
            "   Apparel       Books                    1\n",
            "      Home       Books                    1\n",
            "\n",
            "==================================================\n",
            "--- 2. Temporal Weighting/Recency Decay (Bonus A Logic) ---\n",
            "Re-running prediction for E_S_1_2 using temporalWeight...\n",
            "\n",
            "Top 5 Predictions (Ranked by Temporal Weight):\n",
            " Rank Predicted Event ID Recency Score\n",
            "    1            E_S_1_3        0.0909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQNAdq3j6tOm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}